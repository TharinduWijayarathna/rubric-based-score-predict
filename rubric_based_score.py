# -*- coding: utf-8 -*-
"""rubric_based_score.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Df4Gz8kv_91GHCidLbU4UQ5MsnlieAr
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import cross_val_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    accuracy_score,
    f1_score
)

import joblib
import warnings
warnings.filterwarnings("ignore")

sns.set(style="whitegrid")

train = pd.read_csv("train_dataset.csv")
test  = pd.read_csv("test_dataset.csv")

train.head(), test.head()

target_col = train.columns[-1]

X_train = train.drop(columns=[target_col])
y_train = train[target_col]

X_test = test.copy()

target_col

if y_train.dtype == "object" or y_train.nunique() <= 10:
    problem_type = "classification"
else:
    problem_type = "regression"

problem_type

numeric_features = X_train.select_dtypes(include="number").columns.tolist()
categorical_features = X_train.select_dtypes(exclude="number").columns.tolist()

numeric_features, categorical_features

sns.histplot(y_train, kde=True)
plt.title("Target Distribution (Train Data)")
plt.show()

num_features_len = len(numeric_features)
fig, axes = plt.subplots(num_features_len, 2, figsize=(15, 5 * num_features_len))

for i, col in enumerate(numeric_features):
    # Histogram
    sns.histplot(X_train[col], kde=True, ax=axes[i, 0])
    axes[i, 0].set_title(f"Distribution of {col}")

    # Boxplot
    sns.boxplot(x=X_train[col], ax=axes[i, 1])
    axes[i, 1].set_title(f"Outliers in {col}")

plt.tight_layout()
plt.show()

for col in categorical_features:
    X_train[col].value_counts().plot(kind="bar")
    plt.title(f"Distribution of {col}")
    plt.show()

if len(numeric_features) > 1:
    sns.heatmap(train[numeric_features].corr(), # Removed target_col from here
                cmap="coolwarm", annot=False)
    plt.title("Correlation Matrix (Train Data)")
    plt.show()

numeric_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer([
    ("num", numeric_pipeline, numeric_features),
    ("cat", categorical_pipeline, categorical_features)
])

if problem_type == "regression":
    models = {
        "LinearRegression": LinearRegression(),
        "RandomForest": RandomForestRegressor(random_state=42)
    }
else:
    models = {
        "LogisticRegression": LogisticRegression(max_iter=1000),
        "RandomForest": RandomForestClassifier(random_state=42)
    }

models

results = {}

for name, model in models.items():
    pipe = Pipeline([
        ("preprocessor", preprocessor),
        ("model", model)
    ])

    if problem_type == "regression":
        score = cross_val_score(
            pipe, X_train, y_train,
            cv=5, scoring="neg_root_mean_squared_error"
        ).mean()
        results[name] = -score
    else:
        score = cross_val_score(
            pipe, X_train, y_train,
            cv=5, scoring="f1_weighted"
        ).mean()
        results[name] = score

results

best_model_name = (
    min(results, key=results.get)
    if problem_type == "regression"
    else max(results, key=results.get)
)

best_model = models[best_model_name]

best_model_name

final_pipeline = Pipeline([
    ("preprocessor", preprocessor),
    ("model", best_model)
])

final_pipeline.fit(X_train, y_train)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.histplot(y_train, kde=False, discrete=True)
plt.title('Distribution of Final Grade (Training Data)')
plt.xlabel('Final Grade')
plt.ylabel('Count')

plt.subplot(1, 2, 2)
sns.histplot(test_predictions, kde=False, discrete=True)
plt.title('Distribution of Test Predictions')
plt.xlabel('Predicted Final Grade')
plt.ylabel('Count')

plt.tight_layout()
plt.show()

num_features_len = len(numeric_features)
fig, axes = plt.subplots(num_features_len, 1, figsize=(10, 5 * num_features_len))

for i, col in enumerate(numeric_features):
    sns.boxplot(x=y_train, y=X_train[col], ax=axes[i])
    axes[i].set_title(f'Relationship between {col} and Final Grade')
    axes[i].set_xlabel('Final Grade')
    axes[i].set_ylabel(col)

plt.tight_layout()
plt.show()

best_model = final_pipeline_loaded.named_steps['model']

if hasattr(best_model, 'feature_importances_'):
    # Get feature names from the preprocessor
    # In this case, only numeric features are present and scaled
    feature_names = numeric_features

    # Get feature importances
    importances = best_model.feature_importances_

    # Create a DataFrame for better visualization
    feature_importances_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importances
    })

    # Sort by importance
    feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)

    # Plot feature importances
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=feature_importances_df)
    plt.title('Feature Importances from Best Model')
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.tight_layout()
    plt.show()
else:
    print(f"The best model ({best_model_name}) does not have feature importances.")

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.heatmap(train.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Values in Training Data')

plt.subplot(1, 2, 2)
sns.heatmap(test.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Values in Test Data')

plt.tight_layout()
plt.show()

X_train[numeric_features].describe()

for col in numeric_features:
    plt.figure(figsize=(10, 6))
    sns.violinplot(x=y_train, y=X_train[col], inner="quartile")
    plt.title(f'Violin Plot of {col} vs. Final Grade')
    plt.xlabel('Final Grade')
    plt.ylabel(col)
    plt.show()

print('Value counts for y_train (final_grade in training data):\n', y_train.value_counts())
print('\nValue counts for test_predictions:\n', pd.Series(test_predictions).value_counts())

# Calculate 'total_score' for X_test, assuming it's the sum of q1_score to q5_score
X_test['total_score'] = X_test[['q1_score', 'q2_score', 'q3_score', 'q4_score', 'q5_score']].sum(axis=1)

X_test.head()

test_predictions = final_pipeline.predict(X_test)

test_predictions[:10]

submission = pd.DataFrame({
    "prediction": test_predictions
})

submission.to_csv("submission.csv", index=False)
submission.head()

joblib.dump(final_pipeline, "final_model.pkl")
print("Model saved successfully.")